# AutoConfig

This project demonstrates how to collect system facts from hosts using Ansible
and render the results in a small React application. The playbook gathers the
list of system users, listening network ports and several basic metrics such as
CPU load, memory usage and disk space. The collected data is saved as JSON. A
Python helper script located in the `scripts/` directory runs the playbook and
generates a web page using templates stored in `templates/`.

## Requirements
- Python 3.11
- Ansible
- Jinja2 (installed automatically with Ansible)
- Flask
- pytest (for running the tests)
- docker and docker-compose
- nginx (optional, installed automatically by `setup.sh`)
- See `requirements.txt` for the full Python dependencies

## Setup
Run the provided helper script to automatically install the system packages and
Python requirements. It is recommended to create a virtual environment first:

```bash
python3 -m venv venv
source venv/bin/activate
./setup.sh
pip3 install -r requirements.txt
```

## Usage
Run the helper script which will invoke the playbook, generate a small React
site in the `results` directory and launch a local nginx server to serve it
remotely. If `ansible-playbook` is not available the script will collect facts
from the local machine only so it can be used for quick testing without a full
Ansible setup:

```bash
python3 scripts/collect_and_visualize.py \
  --output-dir results \
  --inventory ansible/hosts.ini \
  --port 8080
```

After the script completes, visit `http://localhost:8080` to view the collected
information. The page now loads host information via a small Flask API instead
of reading a static JSON file. The React interface supports searching, sorting
by columns and filtering by CPU load. Data refreshes automatically every
30&nbsp;seconds without reloading the page. The nginx configuration file is
written to `results/nginx.conf`.

Alternatively you can run the API directly using:

```bash
python3 server.py
```

The API listens on port 5000 by default and serves host data from the
`results` directory using a lightweight SQLite database. Be sure to run the
helper script at least once so that the `results` directory is populated.

### Logging
Both the helper script and the Flask API use Python's `logging` module. Set
the `LOG_LEVEL` environment variable to change verbosity, for example:

```bash
LOG_LEVEL=DEBUG python3 scripts/collect_and_visualize.py
```

## Tests
Install development dependencies and run the unit tests with `pytest`:

```bash
pip install -r requirements.txt
pytest
```

The CI workflow located in `.github/workflows/ci.yml` automatically executes
these tests on every push and pull request.

## Docker

Build the image:

```bash
docker build -t autocfg .
```

Ensure the `results` directory contains data generated by the helper script:

```bash
python3 scripts/collect_and_visualize.py \
  --output-dir results \
  --inventory ansible/hosts.ini \
  --port 8080
```

Run the container and mount the results directory:

```bash
docker run -p 5000:5000 -v $(pwd)/results:/app/results autocfg
```

### Using docker-compose

A sample `docker-compose.yml` is provided to launch the Flask API together with
an nginx container that serves the static site and proxies API requests. The
`flask` service is built from the local `Dockerfile` and shares the `results`
directory with the `nginx` container.

Start both services with:

```bash
docker-compose up --build
```

Alternatively you can run the helper script `run_local.sh` which collects
the host data and launches the compose stack in one step:

```bash
./run_local.sh
```

The site will be available on [http://localhost:8080](http://localhost:8080).

### Publishing the image

To publish the API image to a registry like Docker Hub run:

```bash
docker build -t yourname/autocfg:latest .
docker push yourname/autocfg:latest
```

The compose file can then reference the pushed tag instead of building the
image locally.
